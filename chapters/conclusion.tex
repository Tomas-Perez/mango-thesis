\chapter{Conclusions and Future Work} \label{ch:Conclusions}

In this Chapter we provide a final overview of our work, as well as some future challenges for MANGO according to our vision of its development.

Section \ref{sect:conclusions} contains our conclusions about the work on this thesis, covering our entire journey in chronological order from the initial contribution of the Python wrapper to the final experimental campaign covered in this document.

Section \ref{sect:future-work} is destined to the next developers and researchers working on the system. Here we hope to communicate our vision of the future of MANGO and provide a starting point for improvements. 

\section{Conclusions} \label{sect:conclusions}

In this work we explored our contributions to the MANGO software stack, extending the capabilities of the platform and restructuring its design to improve its future extensibility. This covered a wide range of modifications across most of the MANGOLIBS codebase \cite{mango_repo}, and the addition of multiple new libraries to the system.

Our work started with the implementation of a Python wrapper, aiming to provide users with a simpler API in a language that is widely used for data science and machine learning applications. As these types of applications are one of the main drivers for HPC development, a Python API allows our programming model to be more easily accessible and usable for new users.

After that, the addition of Just-In-Time (JIT) compilation allowed users to load kernels from source code without the need of pre-compilation. This not only improves the feedback loop by decreasing the time to deploy a new kernel modification, but will also allow, in the future, for optimizations over the kernel code which need runtime device capability and availability information.

Our main contribution involved the extension of the MANGO supported platforms, expanding the system with the capability of exploiting GPU accelerators for general purpose applications. This was achieved by the implementation of the Nvidia Architecture Node (NAN), which leverages the CUDA Driver API to execute kernels in any of the connected devices. 

During the integration of the NAN with the existing MANGO stack, there was a significant hurdle when adapting the Libmango code to support this new platform. Hurdle that was a clear consequence of the dependence between Libmango and the underlying architectures running the kernels. Even though the initial abstraction layer of the HN library and daemon allowed for abstraction over the custom MANGO hardware, it was not easily extendable to other devices, such as a GPUs.

This finally concluded in our implementation of the Heterogeneous Hardware Abstraction Library (HHAL), which also involved an internal restructuring of the Libmango library. With the new HHAL library, we aimed to allow for simple extension of the software stack to support new devices, while also maximizing the performance that can be exploited from each individual architecture. By keeping the hardware abstractions at the appropriate level, platform specific improvements can be made without affecting the rest of the interconnected libraries, while still achieving greater performance benefits. Furthermore, the client-server approach used when designing the HHAL will allow for a straightforward adaptation of MANGO into a distributed system, spanning multiple clusters.

Closing out our thesis, we moved forward with an experimental campaign to evaluate the new capabilities added by the NAN and the HHAL libraries, as well as how the MANGO project as a whole compares with other, more established, programming models in terms of performance and programmability. Even though maximizing performance was not our main priority for this initial implementation, the results of our experiments show a promising outline for the future of the project. 

The performance overheads found in MANGO when compared to other programming models were not negligible, but also easily identifiable and, in principle, solvable thanks to the final state of the software stack. To facilitate this, and also to further improve the ease of use and programmability of the system, we include our suggestions and vision for future work in the following section.

\section{Future Work} \label{sect:future-work}

During our work on the project, we thought of multiple ideas to improve the platform, but were not able to cover ourselves in order to keep a reasonable scope for this thesis. Here we detail these ideas, in hope that they are considered by up and coming developers, looking to delve into the MANGO software stack and continue growing the platform.

\subsection{Task Graph Automatic Execution}

In the search of making MANGO's usage as seamless and easy for developers as possible, a great addition to the framework would be the automatic execution of task graphs. 

By automatic execution, we refer to the execution of the entirety of an application's task graph from a single start signal, leaving to MANGO the responsibility of handling buffer data transference and kernel execution from their dependencies.

All the necessary information to make this happen is already being provided by users of Libmango through the task graph definition. As a result, the only Libmango API change this might entail is the addition of a start task graph call.

This functionality can be fully implemented in the Libmango module, which based on its task graph knowledge, is capable of keeping track of kernel dependencies and perform buffer read/write and kernel start calls to the HHAL module as required.

\subsection{GPU Kernel Parallelism Optimization}

When it comes to optimization, there is an endless number of approaches to investigate with the goal of achieving a higher level of system efficiency, as seen in the multiple resource allocation policies that are already implemented in the BarbequeRTRM module.

Focusing on the GPU landscape, optimizing the use of their parallelization capabilities is the key to achieve better results. In 2019, a prototype for workload-aware efficient multiprocessing for modern GPGPUS: Slate \cite{slate}, was presented.
In this work, the authors introduce the idea of modifying kernels source code to allow for control of their parallelization extent at all times, while also maintaining their correct functionality. This way, the GPU resources (i.e. number of cores) an executing kernel is using can be modified "on the go", allowing the reconfiguration of allocated GPU resources to either reduce or increase the number of running tasks at any certain time. This job is carried out by a system-aware resource manager akin to MANGO's BarbequeRTRM.

Since BarbequeRTRM does not currently support reconfiguration of allocated resources once an application is running, there is clearly a great amount of work to be done on this front before a Slate-type optimization could be implemented.

\subsection{Native CPU Support}

In the current implementation, the GN module is used to run kernels on the user's host system CPU. The GN module was developed as an HN emulator and therefore its execution model is not optimized for efficiently running kernels on CPU devices, as its API is limited to that of HN and mimics its behavior.

To properly support CPUs as target accelerators, a new architecture node must be introduced, along with an HHAL manager that interacts with it.

\subsection{HNlib Integration}

The restructuring of the MANGO software stack, centered around the introduction of the Heterogeneous Hardware Abstraction Layer, forced a rework of the Libmango module. This entailed the removal of the Libmango-HNlib interaction, hence effectively eliminating the HN accelerators from the pool of MANGO supported architectures.
With accelerator communication now taking place through the HHAL module, an HN Manager should be added to HHAL in order to reestablish it.
As we did not have physical access to the custom MANGO hardware, designed to work with the HN library, no integration effort was carried out.

\subsection{HHAL "hybrid" mode communication} \label{sub-sect:hhal-hybrid-mode}

As seen in our experimental results in chapter \ref{ch:ExperimentalResults}, the performance of MANGO is hurt by the client-server approach of the HHAL. Although this aspect is key for a distributed system, it can be improved when dealing with a system that runs the entire MANGO stack on a single cluster. 

Our proposed "hybrid" mode introduces a new type of communication between Libmango, the HHAL and BarbequeRTRM, provided that they all run on the same machine. For necessary bookkeeping between BarbequeRTRM and other modules, the solution would switch from pure socket connections to a mix of shared memory and socket usage. While all types of IPC would be bypassed when dealing with kernel executions and buffer transfers, by directly linking HHAL to Libmango.

This solution would provide a significant performance improvement for memory-bound applications in cases where socket communication is not required.

\subsection{Multithreaded CUDA Management}

In order to improve the performance of MANGO when running CUDA applications, it is important to implement multithreading in the Nvidia Architecture Node library running on the HHAL server. As mentioned in chapter \ref{ch:ExperimentalResults}, it would be possible to avoid the overhead of switching device contexts between each kernel launch by creating a separate thread for each CUDA device. 

In addition, if the CUDA stream API is used, it would allow for the asynchronous transfer of data to and from device buffers, consequently enabling the overlap of data transfers and kernel executions. 

By carrying out this implementation on the HHAL module, the changes would be completely transparent to Libmango, and by extension, the user.
