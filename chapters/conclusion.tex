\chapter{Conclusions and Future Work} \label{ch:Conclusions}

% TODO Chapter introduction

\section{Conclusions}

\section{Future Work}
\subsection{Task Graph Automatic Execution}

In the search of making MANGO's usage as seamless and easy for developers as possible, a great addition to the framework would be the automatic execution of task graphs. 

By automatic execution, we refer to the execution of the entirety of an application's task graph from a single start signal, leaving to MANGO the responsibility of handling buffer data transference and kernel execution from their dependencies.

All the necessary information to make this happen is already being provided by users of Libmango through the task graph definition. As a result, the only Libmango API change this might entail is the addition of a start task graph call.

This functionality can be fully implemented in the Libmango module, which based on its task graph knowledge, is capable of keeping track of kernel dependencies and perform buffer read/write and kernel start calls to the HHAL module as required.

\subsection{GPU Kernel Parallelism Optimization}

When it comes to optimization, there is an endless number of approaches to investigate with the goal of achieving a higher level of system efficiency, as seen in the multiple resource allocation policies that were already implemented in the BarbequeRTRM module.

Focusing on the GPU landscape, optimizing the use of their parallelization capabilities is the key to achieve better results. In 2019, a prototype for workload-aware efficient multiprocessing for modern GPGPUS \cite{slate}: Slate, was presented.
In this work, the authors introduce the idea of modifying kernels source code to allow for control of their parallelization extent at all times, while also maintaining their correct functionality. This way, the GPU resources (i.e. number of cores) an executing kernel is using can be modified "on the go", allowing the reconfiguration of allocated GPU resources to either reduce or augment the number of running tasks at any certain time. This job is carried out by a system-aware resource manager akin to MANGO's BarbequeRTRM.

Since BarbequeRTRM does not currently support reconfiguration of allocated resources once an application is running, there is clearly a great amount of work to be done on this front before a Slate-type optimization could be implemented.

\subsection{Native CPU Support}

In the current implementation, the GN module is used to run kernels in the user's host system CPU. The GN module was developed as an HN emulator, so its working method is therefore not optimized for efficiently running kernels on CPU devices, as its API is limited to that of HN.

To properly support CPUs as target accelerators, a new architecture node must be introduced, along with an HHAL manager that interacts with it.

\subsection{HNlib Integration}

The restructuring of the MANGO software stack, centered around the introduction of the Heterogeneous Hardware Abstraction Layer, forced a rework of the Libmango module. This entailed the removal of the Libmango-HNlib interaction, hence effectively eliminating the HN accelerators from the pool of MANGO supported architectures.
With accelerator communication now taking place through the HHAL module, an HN Manager should be added to HHAL in order to reestablish it.

\subsection{HHAL "mixed-mode" communication} \label{sub-sect:hhal-mixed-mode}

As seen in our experimental results in chapter \ref{ch:ExperimentalResults}, the performance of MANGO is hurt by the client-server approach of the HHAL. Although this aspect is key for a distributed system, it can be improved when dealing with a computer which runs both the MANGO host and the HHAL. 

Our proposed "mixed-mode" will introduce a new type of communication between, most importantly the MANGO host and the HHAL, but also Barbeque if it runs on the same machine. It will switch from pure socket connections to a mix of shared memory, for necessary bookkeeping between Barbeque and MANGO, and a library directly linked to the MANGO executable, bypassing all types of IPC when dealing with kernel executions and buffer transfers.

This will provide a significant performance improvement for memory-bound applications in the cases where socket communication is not required.
