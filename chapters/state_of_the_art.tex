\chapter{State of the Art}

\section{OpenCL}
OpenCL is an open standard for cross-platform, parallel programming of diverse accelerators. 

This standard provides a language based on C/C++ which can be used to code kernels that can be executed on any device that supports it. Depending on the target device, the kernel will be passed through a different compiler in order to generate the corresponding instructions. 

In conjunction with the language, the standard also defines a framework for managing the available devices. It consists of a Platform Layer API to query, select and initialize compute devices, and a Runtime API which handles kernel building and execution \cite{opencl_overview}. 

Today, OpenCL is at the forefront of heterogeneous architecture programming. Seventeen different companies (including Apple, Intel, AMD and Nvidia) distribute products conforming to the standard \cite{opencl_conformant_companies}. This allows an OpenCL kernel to run on the majority of hardware available on the market.

Due to this popularity, a significant amount papers have been published looking into different types of extensions on the OpenCL framework.

\subsection{Kernel Scheduling}
Multiple different techniques are discussed in order to optimize the scheduling of the kernels on the devices available on the platform. The overall objective always being minizing the turnaround time of a kernel or group of kernels.

\cite{transparent_cpu_gpu_collaboration} proposes the SKMD (Single Kernel Multiple Devices) system, where the workload of a single kernel can be split among all the available devices. At runtime, a partitioning algorithm will determine how to make this split, based on memory access pattern analysis of the kernel and previous profiling data. An effective partitioning makes sure that the performance improvements outweigh the overhead of extra memory transfers to the different devices. 

Machine learning is an alternative used to forgo the need of profiling the kernels before scheduling. \cite{smart_multitasking_scheduling} achieves this by implementing a model to predict the kernel speedup on a particular device based on static analysis of the code. Also, instead of focusing on a single kernel, it manages scheduling of multiple kernels, possibly belonging to different applications. A very similar approach is also proposed by \cite{load_balance_model_opencl_integrated_cluster}.
