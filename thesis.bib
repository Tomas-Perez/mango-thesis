@misc{opencl_overview,
	title = {{OpenCL: Overview}},
	note = {\url{https://www.khronos.org/opencl/}, Accessed: Apr 2021}
}

@misc{opencl_conformant_companies,
	title = {{OpenCL: API Adopters}},
	note = {\url{https://www.khronos.org/conformance/adopters/conformant-companies}, Accessed: Apr 2021}
}

@misc{openmp_spec,
	title = {{OpenMP API Specification 5.1}},
	year = {2020},
	month = {Nov},
	note = {\url{https://www.openmp.org/wp-content/uploads/OpenMP-API-Specification-5-1.pdf}}
}

@misc{openmp_gpu_support,
	title = {{OpenMP Accelerator Support for GPUs}},
	note = {\url{https://www.openmp.org/updates/openmp-accelerator-support-gpus/}, Accessed: Apr 2021}
}

@misc{openacc_spec,
	title = {{OpenACC API Specification 3.1}},
	year = {2020},
	month = {Nov},
	note = {\url{https://www.openacc.org/sites/default/files/inline-images/Specification/OpenACC-3.1-final.pdf}}
}

@misc{openacc_initial_spec,
	title = {{OpenACC API Specification 1.0}},
	year = {2011},
	month = {Nov},
	note = {\url{https://www.openacc.org/sites/default/files/inline-files/OpenACC_1_0_specification.pdf}}
}

@inproceedings{transparent_cpu_gpu_collaboration,
	author = {Lee, Janghaeng and Samadi, Mehrzad and Park, Yongjun and Mahlke, Scott},
	title = {Transparent CPU-GPU Collaboration for Data-Parallel Kernels on Heterogeneous Systems},
	year = {2013},
	isbn = {9781479910212},
	publisher = {IEEE Press},
	abstract = {Heterogeneous computing on CPUs and GPUs has traditionally used fixed roles for each device: the GPU handles data parallel work by taking advantage of its massive number of cores while the CPU handles non data-parallel work, such as the sequential code or data transfer management. Unfortunately, this work distribution can be a poor solution as it under utilizes the CPU, has difficulty generalizing beyond the single CPU-GPU combination, and may waste a large fraction of time transferring data. Further, CPUs are performance competitive with GPUs on many workloads, thus simply partitioning work based on the fixed roles may be a poor choice. In this paper, we present the single kernel multiple devices (SKMD) system, a framework that transparently orchestrates collaborative execution of a single data-parallel kernel across multiple asymmetric CPUs and GPUs. The programmer is responsible for developing a single data-parallel kernel in OpenCL, while the system automatically partitions the workload across an arbitray set of devices, generates kernels to execute the partial workloads, and efficiently merges the partial outputs together. The goal is performance improvement by maximally utilizing all available resources to execute the kernel. SKMD handles the difficult challenges of exposed data transfer costs and the performance variations GPUs have with respect to input size. On real hardware, SKMD achieves an average speedup of 29% on a system with one multicore CPU and two asymmetric GPUs compared to a fastest device execution strategy for a set of popular OpenCL kernels.},
	booktitle = {Proceedings of the 22nd International Conference on Parallel Architectures and Compilation Techniques},
	pages = {245–256},
	numpages = {12},
	keywords = {collaboration, openCL, GPGPU, data parallel},
	location = {Edinburgh, Scotland, UK},
	series = {PACT '13}
}

@inproceedings{smart_multitasking_scheduling,
	author={Y. {Wen} and Z. {Wang} and M. F. P. {O'Boyle}},
	booktitle={2014 21st International Conference on High Performance Computing (HiPC)}, 
	title={Smart multi-task scheduling for OpenCL programs on CPU/GPU heterogeneous platforms}, 
	year={2014},
	volume={},
	number={},
	pages={1-10},
	doi={10.1109/HiPC.2014.7116910}
}

@Article{load_balance_model_opencl_integrated_cluster,
	author={Ahmed, Usman
	and Lin, Jerry Chun-Wei
	and Srivastava, Gautam
	and Aleem, Muhammad},
	title={A load balance multi-scheduling model for OpenCL kernel tasks in an integrated cluster},
	journal={Soft Computing},
	year={2021},
	month={Jan},
	day={01},
	volume={25},
	number={1},
	pages={407-420},
	abstract={Nowadays, embedded systems are comprised of heterogeneous multi-core architectures, i.e., CPUs and GPUs. If the application is mapped to an appropriate processing core, then these architectures provide many performance benefits to applications. Typically, programmers map sequential applications to CPU and parallel applications to GPU. The task mapping becomes challenging because of the usage of evolving and complex CPU- and GPU-based architectures. This paper presents an approach to map the OpenCL application to heterogeneous multi-core architecture by determining the application suitability and processing capability. The classification is achieved by developing a machine learning-based device suitability classifier that predicts which processor has the highest computational compatibility to run OpenCL applications. In this paper, 20 distinct features are proposed that are extracted by using the developed LLVM-based static analyzer. In order to select the best subset of features, feature selection is performed by using both correlation analysis and the feature importance method. For the class imbalance problem, we use and compare synthetic minority over-sampling method with and without feature selection. Instead of hand-tuning the machine learning classifier, we use the tree-based pipeline optimization method to select the best classifier and its hyper-parameter. We then compare the optimized selected method with traditional algorithms, i.e., random forest, decision tree, Na{\"i}ve Bayes and KNN. We apply our novel approach on extensively used OpenCL benchmarks, i.e., AMD and Polybench. The dataset contains 653 training and 277 testing applications. We test the classification results using four performance metrics, i.e., F-measure, precision, recall and {\$}{\$}R^2{\$}{\$}. The optimized and reduced feature subset model achieved a high F-measure of 0.91 and {\$}{\$}R^2{\$}{\$}of 0.76. The proposed framework automatically distributes the workload based on the application requirement and processor compatibility.},
	issn={1433-7479},
	doi={10.1007/s00500-020-05152-8},
	url={https://doi.org/10.1007/s00500-020-05152-8}
}

@article{dynamic_self_scheduling,
	author = {Belviranli, Mehmet E. and Bhuyan, Laxmi N. and Gupta, Rajiv},
	title = {A Dynamic Self-Scheduling Scheme for Heterogeneous Multiprocessor Architectures},
	year = {2013},
	issue_date = {January 2013},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {9},
	number = {4},
	issn = {1544-3566},
	url = {https://doi.org/10.1145/2400682.2400716},
	doi = {10.1145/2400682.2400716},
	abstract = {Today's heterogeneous architectures bring together multiple general-purpose CPUs and multiple domain-specific GPUs and FPGAs to provide dramatic speedup for many applications. However, the challenge lies in utilizing these heterogeneous processors to optimize overall application performance by minimizing workload completion time. Operating system and application development for these systems is in their infancy.In this article, we propose a new scheduling and workload balancing scheme, HDSS, for execution of loops having dependent or independent iterations on heterogeneous multiprocessor systems. The new algorithm dynamically learns the computational power of each processor during an adaptive phase and then schedules the remainder of the workload using a weighted self-scheduling scheme during the completion phase. Different from previous studies, our scheme uniquely considers the runtime effects of block sizes on the performance for heterogeneous multiprocessors. It finds the right trade-off between large and small block sizes to maintain balanced workload while keeping the accelerator utilization at maximum. Our algorithm does not require offline training or architecture-specific parameters.We have evaluated our scheme on two different heterogeneous architectures: AMD 64-core Bulldozer system with nVidia Fermi C2050 GPU and Intel Xeon 32-core SGI Altix 4700 supercomputer with Xilinx Virtex 4 FPGAs. The experimental results show that our new scheduling algorithm can achieve performance improvements up to over 200% when compared to the closest existing load balancing scheme. Our algorithm also achieves full processor utilization with all processors completing at nearly the same time which is significantly better than alternative current approaches.},
	journal = {ACM Trans. Archit. Code Optim.},
	month = jan,
	articleno = {57},
	numpages = {20},
	keywords = {workload balancing, FPGAs, GP-GPUs, Dynamic self-scheduling}
}

@article{enginecl,
	author = {Nozal, Raúl and Bosque, Jose and Beivide, Ramon},
	year = {2020},
	month = {02},
	pages = {},
	title = {EngineCL: Usability and Performance in Heterogeneous Computing},
	volume = {107},
	journal = {Future Generation Computer Systems},
	doi = {10.1016/j.future.2020.02.016}
}

@inproceedings{fluidicl,
	author = {Pandit, Prasanna and Govindarajan, R.},
	title = {Fluidic Kernels: Cooperative Execution of OpenCL Programs on Multiple Heterogeneous Devices},
	year = {2014},
	isbn = {9781450326704},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2581122.2544163},
	doi = {10.1145/2581122.2544163},
	abstract = {Programming heterogeneous computing systems with Graphics Processing Units (GPU) and multi-core CPUs in them is complex and time-consuming. OpenCL has emerged as an attractive programming framework for heterogeneous systems. But utilizing multiple devices in OpenCL is a challenge because it requires the programmer to explicitly map data and computation to each device. The problem becomes even more complex if the same OpenCL kernel has to be executed synergistically using multiple devices, as the relative execution time of the kernel on different devices can vary significantly, making it difficult to determine the work partitioning across these devices a priori. Also, after each kernel execution, a coherent version of the data needs to be established.In this work, we present FluidiCL, an OpenCL runtime that takes a program written for a single device and uses both the CPU and the GPU to execute it. Since we consider a setup with devices having discrete address spaces, our solution ensures that execution of OpenCL work-groups on devices is adjusted by taking into account the overheads for data management. The data transfers and data merging needed to ensure coherence are handled transparently without requiring any effort from the programmer. FluidiCL also does not require prior training or profiling and is completely portable across different machines. Across a set of diverse benchmarks having multiple kernels, our runtime shows a geomean speedup of nearly 64% over a high-end GPU and 88% over a 4-core CPU. In all benchmarks, performance of our runtime comes to within 13% of the best of the two devices.},
	booktitle = {Proceedings of Annual IEEE/ACM International Symposium on Code Generation and Optimization},
	pages = {273–283},
	numpages = {11},
	keywords = {Heterogeneous Devices, GPGPU, Runtime, OpenCL, FluidiCL},
	location = {Orlando, FL, USA},
	series = {CGO '14}
}

@inproceedings{survey_programming_models,
  author={B. {Pervan} and J. {Knezović}},
  booktitle={2020 43rd International Convention on Information, Communication and Electronic Technology (MIPRO)}, 
  title={A Survey on Parallel Architectures and Programming Models}, 
  year={2020},
  volume={},
  number={},
  pages={999-1005},
  doi={10.23919/MIPRO48935.2020.9245341}
}

@inproceedings{openmp_vs_openacc,
	author="Wienke, Sandra
	and Terboven, Christian
	and Beyer, James C.
	and M{\"u}ller, Matthias S.",
	editor="Silva, Fernando
	and Dutra, In{\^e}s
	and Santos Costa, V{\'i}tor",
	title="A Pattern-Based Comparison of OpenACC and OpenMP for Accelerator Computing",
	booktitle="Euro-Par 2014 Parallel Processing",
	year="2014",
	publisher="Springer International Publishing",
	address="Cham",
	pages="812--823",
	abstract="Nowadays, HPC systems frequently emerge as clusters of commodity processors with attached accelerators. Moving from tedious low-level accelerator programming to increased development productivity, the directive-based programming models OpenACC and OpenMP are promising candidates. While OpenACC was completed about two years ago, OpenMP just recently added support for accelerator programming. To assist developers in their decision-making which approach to take, we compare both models with respect to their programmability. Besides investigating their expressiveness by putting their constructs side by side, we focus on the evaluation of their power based on structured parallel programming patterns (aka algorithmic skeletons). These patterns describe the basic entities of parallel algorithms of which we cover the patterns map, stencil, reduction, fork-join, superscalar sequence, nesting and geometric decomposition. Architectural targets of this work are NVIDIA-type accelerators (GPUs) and specialties of Intel-type accelerators (Xeon Phis). Additionally, we assess the prospects of OpenACC and OpenMP concerning future development in soft- and hardware design.",
	isbn="978-3-319-09873-9"
}
