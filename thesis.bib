@misc{opencl_overview,
  title = {{OpenCL: Overview}},
  note  = {\url{https://www.khronos.org/opencl/}, Accessed: Apr 2021}
}

@misc{opencl_conformant_companies,
  title = {{OpenCL: API Adopters}},
  note  = {\url{https://www.khronos.org/conformance/adopters/conformant-companies}, Accessed: Apr 2021}
}

@misc{opencl_spec,
  title = {{The OpenCL Specification v3.0.6}},
  year  = {2020},
  month = {Dec},
  note  = {\url{https://www.khronos.org/registry/OpenCL/specs/3.0-unified/pdf/OpenCL_API.pdf}}
}

@misc{opencl_c_spec,
  title = {{The OpenCL C Specification v3.0.6}},
  year  = {2020},
  month = {Dec},
  note  = {\url{https://www.khronos.org/registry/OpenCL/specs/3.0-unified/pdf/OpenCL_C.pdf}}
}

@misc{c99,
  title = {{ISO/IEC 9899:1999 - Programming languages - C}},
  year  = {1999},
  month = {Dec}
}

@misc{c11,
  title = {{ISO/IEC 9899:2011 - Information technology - Programming languages - C}},
  year  = {2011},
  month = {Dec}
}

@misc{openmp_spec,
  title = {{OpenMP API Specification 5.1}},
  year  = {2020},
  month = {Nov},
  note  = {\url{https://www.openmp.org/wp-content/uploads/OpenMP-API-Specification-5-1.pdf}}
}

@misc{openmp_gpu_support,
  title = {{OpenMP Accelerator Support for GPUs}},
  note  = {\url{https://www.openmp.org/updates/openmp-accelerator-support-gpus/}, Accessed: Apr 2021}
}

@misc{openacc_spec,
  title = {{OpenACC API Specification 3.1}},
  year  = {2020},
  month = {Nov},
  note  = {\url{https://www.openacc.org/sites/default/files/inline-images/Specification/OpenACC-3.1-final.pdf}}
}

@misc{openacc_initial_spec,
  title = {{OpenACC API Specification 1.0}},
  year  = {2011},
  month = {Nov},
  note  = {\url{https://www.openacc.org/sites/default/files/inline-files/OpenACC_1_0_specification.pdf}}
}

@misc{spir_spec,
  title = {{SPIR-V Specification 1.5 Revision 5}},
  year  = {2020},
  month = {Jan},
  note  = {\url{https://www.khronos.org/registry/spir-v/specs/unified1/SPIRV.html}}
}

@misc{spir_overview,
  title = {{SPIR Overview}},
  note  = {\url{https://www.khronos.org/spir/}, Accessed: Apr 2021}
}

@misc{vulkan,
  title = {{Vulkan}},
  note  = {\url{https://www.khronos.org/vulkan/}, Accessed: Apr 2021}
}

@misc{brief_history_gpgpu,
  title  = {{A Brief History of GPGPU}},
  author = {{Mark Harris}},
  month  = {May},
  year   = {2015}
}

@misc{ikonas,
  title  = {{Ikonas Graphics Systems - The world's first GPGPU}},
  author = {{Nick England}},
  note   = {\url{http://www.graphics-history.org/ikonas/}, Accessed: Apr 2021}
}

@misc{geforce_256,
  title = {{TechPowerUp: NVIDIA GeForce 256 SDR}},
  note  = {\url{https://www.techpowerup.com/gpu-specs/geforce-256-sdr.c731}, Accessed: Apr 2021}
}

@misc{ati_9700_pro,
  title = {{TechPowerUp: ATI Radeon 9700 PRO}},
  note  = {\url{https://www.techpowerup.com/gpu-specs/radeon-9700-pro.c50}, Accessed: Apr 2021}
}

@misc{geforcefx,
  title = {{TechPowerUp: NVIDIA GeForceFX}},
  note  = {\url{https://www.techpowerup.com/gpu-specs/radeon-9700-pro.c50}, Accessed: Apr 2021}
}

@misc{opengl,
  title = {{OpenGL}},
  note  = {\url{https://www.opengl.org/}, Accessed: Apr 2021}
}

@misc{opengl_fixed_function_pipeline,
  title = {{OpenGL Wiki: Fixed Function Pipeline}},
  note  = {\url{https://www.khronos.org/opengl/wiki/Fixed_Function_Pipeline}, Accessed: Apr 2021}
}

@misc{nvidia_nfinitefx_pixel,
  title = {{NVIDIA nfiniteFX Engine: Programmable Pixel Shaders}},
  year  = {2001}
}

@misc{nvidia_nfinitefx_vertex,
  title = {{NVIDIA nfiniteFX Engine: Programmable Vertex Shaders}},
  year  = {2001}
}

@misc{gamasutra_cg_release,
  title = {{Gamasutra: Nvidia Releases Cg Compiler 1.0}},
  note  = {\url{https://www.gamasutra.com/view/news/92570/Nvidia_Releases_Cg_Compiler_10.php}, Accessed: Apr 2021}
}

@misc{xbox_360_specs,
  title = {{Xbox 360 Technical Specifications (Wayback Machine Archive Aug 2008}},
  note  = {\url{https://web.archive.org/web/20080822024003/http://www.xbox.com/en-AU/support/xbox360/manuals/xbox360specs.htm}, Accessed: Apr 2021}
}

@misc{geforce_8800_architecture,
  title = {{NVIDIA GeForce 8800 GPU Architecture Overview}},
  month = {Nov},
  year  = {2006},
  note  = {\url{https://www.nvidia.co.uk/content/PDF/Geforce_8800/GeForce_8800_GPU_Architecture_Technical_Brief.pdf}, Accessed: Apr 2021}
}

@misc{cuda_toolkit_archive,
  title = {{CUDA Toolkit Archive}},
  note  = {\url{https://developer.nvidia.com/cuda-toolkit-archive}, Accessed: Apr 2021}
}

@misc{but_mummy_cuda,
  title  = {{But Mummy I don't want to use CUDA - Open source GPU compute}},
  author = {David Airlie},
  month  = {Jan},
  year   = {2019},
  note   = {\url{https://www.youtube.com/watch?v=ZTq8wKnVUZ8}, Accessed: Apr 2021}
}

@misc{cuda_open_source_compiler,
  title  = {{CppCon 2016: “Bringing Clang and C++ to GPUs: An Open-Source, CUDA-Compatible GPU C++ Compiler"}},
  author = {Justin Lebar},
  year   = {2016},
  note   = {\url{https://www.youtube.com/watch?v=KHa-OSrZPGo}, Accessed: Apr 2021}
}

@misc{amd_ctm_programming_guide,
  title = {{AMD Compute Abstraction Layer (CAL) Programming}},
  year  = {2010},
  month = {Dec},
  note  = {\url{https://developer.amd.com/wordpress/media/2012/10/AMD_CAL_Programming_Guide_v2.0.pdf}, Accessed: Apr 2021}
}

@misc{amd_ctm_ditch,
  title = {{Tom's Hardware: AMD Ditches Close-To-Metal, Focuses On DX11 And OpenCL}},
  year  = {2008},
  month = {Aug},
  note  = {\url{https://www.tomshardware.com/news/AMD-stream-processor-GPGPU,6072.html}, Accessed: Apr 2021}
}

@misc{sycl_2020_standard,
  title = {{SYCL 2020 Specification (revision 3)}},
  year = {2021},
  month = {Mar},
  note = {\url{https://www.khronos.org/registry/SYCL/specs/sycl-2020/pdf/sycl-2020.pdf}, Accessed: Apr 2021}
}

@misc{cpp17,
  title = {{ISO/IEC 14882:2017 Programming languages — C++}},
  year  = {2017},
  month = {Dec}
}

@misc{cpp20,
  title = {{ISO/IEC 14882:2020 Programming languages — C++}},
  year  = {2020},
  month = {Dec}
}

@misc{sycl_faq,
  title = {{SYCL 2020 - What you need to know}},
  year  = {2020},
  note  = {\url{https://www.khronos.org/blog/sycl-2020-what-do-you-need-to-know}, Accessed: May 2021}
}

@misc{caffe_rocm_port,
  title = {{SC16: HIP and CAFFE Porting and Profiling with AMD's ROCm}},
  year  = {2016},
  note  = {\url{https://www.youtube.com/watch?v=I7AfQ730Zwc}, Accessed: May 2021}
}

@misc{boltzmann_initiative,
  title = {{AMD Launches ‘Boltzmann Initiative’ to Dramatically Reduce Barriers to GPU Computing on AMD FirePro™ Graphics}},
  month = {Nov},
  year  = {2015},
  note  = {\url{https://www.amd.com/en/press-releases/boltzmann-initiative-2015nov16}, Accessed: May 2021}
}

@inproceedings{transparent_cpu_gpu_collaboration,
  author    = {Lee, Janghaeng and Samadi, Mehrzad and Park, Yongjun and Mahlke, Scott},
  title     = {Transparent CPU-GPU Collaboration for Data-Parallel Kernels on Heterogeneous Systems},
  year      = {2013},
  isbn      = {9781479910212},
  publisher = {IEEE Press},
  abstract  = {Heterogeneous computing on CPUs and GPUs has traditionally used fixed roles for each device: the GPU handles data parallel work by taking advantage of its massive number of cores while the CPU handles non data-parallel work, such as the sequential code or data transfer management. Unfortunately, this work distribution can be a poor solution as it under utilizes the CPU, has difficulty generalizing beyond the single CPU-GPU combination, and may waste a large fraction of time transferring data. Further, CPUs are performance competitive with GPUs on many workloads, thus simply partitioning work based on the fixed roles may be a poor choice. In this paper, we present the single kernel multiple devices (SKMD) system, a framework that transparently orchestrates collaborative execution of a single data-parallel kernel across multiple asymmetric CPUs and GPUs. The programmer is responsible for developing a single data-parallel kernel in OpenCL, while the system automatically partitions the workload across an arbitray set of devices, generates kernels to execute the partial workloads, and efficiently merges the partial outputs together. The goal is performance improvement by maximally utilizing all available resources to execute the kernel. SKMD handles the difficult challenges of exposed data transfer costs and the performance variations GPUs have with respect to input size. On real hardware, SKMD achieves an average speedup of 29% on a system with one multicore CPU and two asymmetric GPUs compared to a fastest device execution strategy for a set of popular OpenCL kernels.},
  booktitle = {Proceedings of the 22nd International Conference on Parallel Architectures and Compilation Techniques},
  pages     = {245–256},
  numpages  = {12},
  keywords  = {collaboration, openCL, GPGPU, data parallel},
  location  = {Edinburgh, Scotland, UK},
  series    = {PACT '13}
}

@inproceedings{smart_multitasking_scheduling,
  author    = {Y. {Wen} and Z. {Wang} and M. F. P. {O'Boyle}},
  booktitle = {2014 21st International Conference on High Performance Computing (HiPC)},
  title     = {Smart multi-task scheduling for OpenCL programs on CPU/GPU heterogeneous platforms},
  year      = {2014},
  volume    = {},
  number    = {},
  pages     = {1-10},
  doi       = {10.1109/HiPC.2014.7116910}
}

@article{load_balance_model_opencl_integrated_cluster,
  author   = {Ahmed, Usman
	and Lin, Jerry Chun-Wei
	and Srivastava, Gautam
	and Aleem, Muhammad},
  title    = {A load balance multi-scheduling model for OpenCL kernel tasks in an integrated cluster},
  journal  = {Soft Computing},
  year     = {2021},
  month    = {Jan},
  day      = {01},
  volume   = {25},
  number   = {1},
  pages    = {407-420},
  abstract = {Nowadays, embedded systems are comprised of heterogeneous multi-core architectures, i.e., CPUs and GPUs. If the application is mapped to an appropriate processing core, then these architectures provide many performance benefits to applications. Typically, programmers map sequential applications to CPU and parallel applications to GPU. The task mapping becomes challenging because of the usage of evolving and complex CPU- and GPU-based architectures. This paper presents an approach to map the OpenCL application to heterogeneous multi-core architecture by determining the application suitability and processing capability. The classification is achieved by developing a machine learning-based device suitability classifier that predicts which processor has the highest computational compatibility to run OpenCL applications. In this paper, 20 distinct features are proposed that are extracted by using the developed LLVM-based static analyzer. In order to select the best subset of features, feature selection is performed by using both correlation analysis and the feature importance method. For the class imbalance problem, we use and compare synthetic minority over-sampling method with and without feature selection. Instead of hand-tuning the machine learning classifier, we use the tree-based pipeline optimization method to select the best classifier and its hyper-parameter. We then compare the optimized selected method with traditional algorithms, i.e., random forest, decision tree, Na{\"i}ve Bayes and KNN. We apply our novel approach on extensively used OpenCL benchmarks, i.e., AMD and Polybench. The dataset contains 653 training and 277 testing applications. We test the classification results using four performance metrics, i.e., F-measure, precision, recall and {\$}{\$}R^2{\$}{\$}. The optimized and reduced feature subset model achieved a high F-measure of 0.91 and {\$}{\$}R^2{\$}{\$}of 0.76. The proposed framework automatically distributes the workload based on the application requirement and processor compatibility.},
  issn     = {1433-7479},
  doi      = {10.1007/s00500-020-05152-8},
  url      = {https://doi.org/10.1007/s00500-020-05152-8}
}

@article{dynamic_self_scheduling,
  author     = {Belviranli, Mehmet E. and Bhuyan, Laxmi N. and Gupta, Rajiv},
  title      = {A Dynamic Self-Scheduling Scheme for Heterogeneous Multiprocessor Architectures},
  year       = {2013},
  issue_date = {January 2013},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {9},
  number     = {4},
  issn       = {1544-3566},
  url        = {https://doi.org/10.1145/2400682.2400716},
  doi        = {10.1145/2400682.2400716},
  abstract   = {Today's heterogeneous architectures bring together multiple general-purpose CPUs and multiple domain-specific GPUs and FPGAs to provide dramatic speedup for many applications. However, the challenge lies in utilizing these heterogeneous processors to optimize overall application performance by minimizing workload completion time. Operating system and application development for these systems is in their infancy.In this article, we propose a new scheduling and workload balancing scheme, HDSS, for execution of loops having dependent or independent iterations on heterogeneous multiprocessor systems. The new algorithm dynamically learns the computational power of each processor during an adaptive phase and then schedules the remainder of the workload using a weighted self-scheduling scheme during the completion phase. Different from previous studies, our scheme uniquely considers the runtime effects of block sizes on the performance for heterogeneous multiprocessors. It finds the right trade-off between large and small block sizes to maintain balanced workload while keeping the accelerator utilization at maximum. Our algorithm does not require offline training or architecture-specific parameters.We have evaluated our scheme on two different heterogeneous architectures: AMD 64-core Bulldozer system with nVidia Fermi C2050 GPU and Intel Xeon 32-core SGI Altix 4700 supercomputer with Xilinx Virtex 4 FPGAs. The experimental results show that our new scheduling algorithm can achieve performance improvements up to over 200% when compared to the closest existing load balancing scheme. Our algorithm also achieves full processor utilization with all processors completing at nearly the same time which is significantly better than alternative current approaches.},
  journal    = {ACM Trans. Archit. Code Optim.},
  month      = jan,
  articleno  = {57},
  numpages   = {20},
  keywords   = {workload balancing, FPGAs, GP-GPUs, Dynamic self-scheduling}
}

@article{enginecl,
  author  = {Nozal, Raúl and Bosque, Jose and Beivide, Ramon},
  year    = {2020},
  month   = {02},
  pages   = {},
  title   = {EngineCL: Usability and Performance in Heterogeneous Computing},
  volume  = {107},
  journal = {Future Generation Computer Systems},
  doi     = {10.1016/j.future.2020.02.016}
}

@inproceedings{fluidicl,
  author    = {Pandit, Prasanna and Govindarajan, R.},
  title     = {Fluidic Kernels: Cooperative Execution of OpenCL Programs on Multiple Heterogeneous Devices},
  year      = {2014},
  isbn      = {9781450326704},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2581122.2544163},
  doi       = {10.1145/2581122.2544163},
  abstract  = {Programming heterogeneous computing systems with Graphics Processing Units (GPU) and multi-core CPUs in them is complex and time-consuming. OpenCL has emerged as an attractive programming framework for heterogeneous systems. But utilizing multiple devices in OpenCL is a challenge because it requires the programmer to explicitly map data and computation to each device. The problem becomes even more complex if the same OpenCL kernel has to be executed synergistically using multiple devices, as the relative execution time of the kernel on different devices can vary significantly, making it difficult to determine the work partitioning across these devices a priori. Also, after each kernel execution, a coherent version of the data needs to be established.In this work, we present FluidiCL, an OpenCL runtime that takes a program written for a single device and uses both the CPU and the GPU to execute it. Since we consider a setup with devices having discrete address spaces, our solution ensures that execution of OpenCL work-groups on devices is adjusted by taking into account the overheads for data management. The data transfers and data merging needed to ensure coherence are handled transparently without requiring any effort from the programmer. FluidiCL also does not require prior training or profiling and is completely portable across different machines. Across a set of diverse benchmarks having multiple kernels, our runtime shows a geomean speedup of nearly 64% over a high-end GPU and 88% over a 4-core CPU. In all benchmarks, performance of our runtime comes to within 13% of the best of the two devices.},
  booktitle = {Proceedings of Annual IEEE/ACM International Symposium on Code Generation and Optimization},
  pages     = {273–283},
  numpages  = {11},
  keywords  = {Heterogeneous Devices, GPGPU, Runtime, OpenCL, FluidiCL},
  location  = {Orlando, FL, USA},
  series    = {CGO '14}
}

@inproceedings{survey_programming_models,
  author    = {B. {Pervan} and J. {Knezović}},
  booktitle = {2020 43rd International Convention on Information, Communication and Electronic Technology (MIPRO)},
  title     = {A Survey on Parallel Architectures and Programming Models},
  year      = {2020},
  volume    = {},
  number    = {},
  pages     = {999-1005},
  doi       = {10.23919/MIPRO48935.2020.9245341}
}

@inproceedings{openmp_vs_openacc,
  author    = {Wienke, Sandra
	and Terboven, Christian
	and Beyer, James C.
	and M{\"u}ller, Matthias S.},
  editor    = {Silva, Fernando
	and Dutra, In{\^e}s
	and Santos Costa, V{\'i}tor},
  title     = {A Pattern-Based Comparison of OpenACC and OpenMP for Accelerator Computing},
  booktitle = {Euro-Par 2014 Parallel Processing},
  year      = {2014},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {812--823},
  abstract  = {Nowadays, HPC systems frequently emerge as clusters of commodity processors with attached accelerators. Moving from tedious low-level accelerator programming to increased development productivity, the directive-based programming models OpenACC and OpenMP are promising candidates. While OpenACC was completed about two years ago, OpenMP just recently added support for accelerator programming. To assist developers in their decision-making which approach to take, we compare both models with respect to their programmability. Besides investigating their expressiveness by putting their constructs side by side, we focus on the evaluation of their power based on structured parallel programming patterns (aka algorithmic skeletons). These patterns describe the basic entities of parallel algorithms of which we cover the patterns map, stencil, reduction, fork-join, superscalar sequence, nesting and geometric decomposition. Architectural targets of this work are NVIDIA-type accelerators (GPUs) and specialties of Intel-type accelerators (Xeon Phis). Additionally, we assess the prospects of OpenACC and OpenMP concerning future development in soft- and hardware design.},
  isbn      = {978-3-319-09873-9}
}

@article{cuda_openacc_openmp_performance,
  doi       = {10.1088/1742-6596/1740/1/012056},
  url       = {https://doi.org/10.1088/1742-6596/1740/1/012056},
  year      = 2021,
  month     = {jan},
  publisher = {{IOP} Publishing},
  volume    = {1740},
  pages     = {012056},
  author    = {Mikhail Khalilov and Alexey Timoveev},
  title     = {Performance analysis of {CUDA}, {OpenACC} and {OpenMP} programming models on {TESLA} V100 {GPU}},
  journal   = {Journal of Physics: Conference Series},
  abstract  = {Graphics processors are widely utilized in modern supercomputers as accelerators. Ability to perform efficient parallelization and low-level allow scientists to greatly boost performance of their codes. Modern Nvidia GPUs feature low-level approaches, such as CUDA, along with high-level approaches: OpenACC and OpenMP. While the low-level approach aims to explore all possible abilities of SIMT GPU architecture by writing low-level C/C++ code, it takes significant effort from programmer. OpenACC and OpenMP programming models are opposite to CUDA. Using these models the programmer only have to identify the blocks of code to be parallelized using pragmas. We compare the performance of CUDA, OpenMP and OpenACC on state-of-the-art Nvidia Tesla V100 GPU in various typical scenarios that arise in scientific programming, such as matrix multiplication, regular memory access patterns and evaluate performance of physical simulation codes implemented using these programming models. Moreover, we study the performance matrix multiplication implemented in vendor-optimized BLAS libraries for Nvidia Tesla V100 GPU and modern Intel Xeon processor.}
}

@article{openmp_openacc_molecular_docking,
  author   = {Vitali, Emanuele
	and Gadioli, Davide
	and Palermo, Gianluca
	and Beccari, Andrea
	and Cavazzoni, Carlo
	and Silvano, Cristina},
  title    = {Exploiting OpenMP and OpenACC to accelerate a geometric approach to molecular docking in heterogeneous HPC nodes},
  journal  = {The Journal of Supercomputing},
  year     = {2019},
  month    = {Jul},
  day      = {01},
  volume   = {75},
  number   = {7},
  pages    = {3374-3396},
  abstract = {In drug discovery, molecular docking is the task in charge of estimating the position of a molecule when interacting with the docking site. This task is usually used to perform screening of a large library of molecules, in the early phase of the process. Given the amount of candidate molecules and the complexity of the application, this task is usually performed using high-performance computing (HPC) platforms. In modern HPC systems, heterogeneous platforms provide a better throughput with respect to homogeneous platforms. In this work, we ported and optimized a molecular docking application to a heterogeneous system, with one or more GPU accelerators, leveraging a hybrid OpenMP and OpenACC approach. The target application focuses on the virtual screening phases in the drug discovery process, and it is based on geometric transformations of the target ligands. We prove that our approach has a better exploitation of the node compared to pure CPU/GPU data splitting approaches, reaching a throughput improvement up to 25{\%} while considering the same computing node.},
  issn     = {1573-0484},
  doi      = {10.1007/s11227-019-02875-w},
  url      = {https://doi.org/10.1007/s11227-019-02875-w}
}

@article{openmp_openacc_multigpus,
  author  = {Xu, Rengan and Chandrasekaran, Sunita and Chapman, Barbara},
  year    = {2013},
  month   = {05},
  pages   = {},
  title   = {Exploring programming multi-GPUS using OpenMP and OpenACC-based hybrid model},
  journal = {Proceedings - IEEE 27th International Parallel and Distributed Processing Symposium Workshops and PhD Forum, IPDPSW 2013},
  doi     = {10.1109/IPDPSW.2013.263}
}

@misc{kernel_wikipedia,
  title = {{Wikipedia: Compute Kernel}},
  note  = {\url{https://en.wikipedia.org/wiki/Compute_kernel}, Accessed: Apr 2021}
}

@misc{buffer_wikipedia,
  title = {{Wikipedia: Data Buffer}},
  note  = {\url{https://en.wikipedia.org/wiki/Data_buffer}, Accessed: Apr 2021}
}

@inproceedings{mango_exploring_manycore_architectures,
  author = {G. Massari and G. Agosta and A. Pupykina and F. Reghenzani and M. Zanella and W. Fornaciari and J. Flich and R. Tornero and J. M. Martinez and M. Zapater and I. P. Fernandez and D. Atienza and A. Cilardo and M. Gagliardi and A. Dray and G. Guillaume},
  year   = {2019},
  month  = {02},
  pages  = {},
  title  = {MANGO - exploring Manycore Architectures for Next-GeneratiOn HPC Systems - WP4}
}

@inproceedings{mango_exploring_manycore_architectures_2.1,
  author = {G. Massari and G. Agosta and A. Pupykina and F. Reghenzani and M. Zanella and W. Fornaciari and J. Flich and R. Tornero and J. M. Martinez and M. Zapater and I. P. Fernandez and D. Atienza and A. Cilardo and M. Gagliardi and A. Dray and G. Guillaume},
  year   = {2018},
  month  = {04},
  day  = {27},
  pages  = {},
  title  = {MANGO - exploring Manycore Architectures for Next-GeneratiOn HPC Systems - WP2}
}

@article{exploring_manycore_architectures_through_the_mango_approach,
  author = {G. Massari and G. Agosta and A. Pupykina and F. Reghenzani and M. Zanella and W. Fornaciari and J. Flich and R. Tornero and J. M. Martinez and M. Zapater and I. P. Fernandez and D. Atienza and A. Cilardo and M. Gagliardi and A. Dray and G. Guillaume},
  year   = {2018},
  month  = {05},
  day  = {15},
  pages  = {6},
  title  = {Exploring Manycore Architectures for Next-Generation HPC Systems through the MANGO Approach},
  journal = {Microprocessors and Microsystems (2018)},
  doi     = {10.1016/j.micpro.2018.05.011}
}

@inproceedings{slate,
  author = {Tyler Allen and Xizhou Feng and Rong Ge},
  year   = {2019},
  pages  = {},
  title  = {Slate: Enabling Workload-Aware Efficient Multiprocessing for Modern GPGPUs}
}

@misc{daemon_wikipedia,
	title = {{Wikipedia: Daemon(computing)}},
	note = {\url{https://en.wikipedia.org/wiki/Daemon_(computing)}, Accessed: Apr 2021}
}

@misc{clang_llvm,
	title = {{Clang: a C language family frontend for LLVM}},
	note = {\url{https://clang.llvm.org}, Accessed: Apr 2021}
}

@misc{redmonks_rankings,
  title= {{The RedMonk Programming Language Rankings: January 2021}},
  note = {\url{https://redmonk.com/sogrady/2021/03/01/language-rankings-1-21/}, Accessed: May 2021}
}

@misc{cython,
  title= {{Cython C-Extensions for Python}},
  note = {\url{https://cython.org}, Accessed: May 2021}
}

@article{barbecue_1,
  author = {Patrick Bellasi and Giuseppe Massari and William Fornaciari},
  year   = {2015},
  month  = {03},
  pages  = {},
  title  = {Effective Runtime Resource Management Using Linux Control Groups with the BarbequeRTRM Framework},
  doi    = {https://doi.org/10.1145/2658990}
}

@article{barbecue_2,
  author = {Patrick Bellasi and Giuseppe Massari and William Fornaciari},
  year   = {2012},
  month  = {},
  pages  = {},
  title  = {A RTRM proposal for multi/many-core platforms and reconfigurable applications},
  doi    = {https://doi.org/10.1109/ReCoSoC.2012.6322885}
}

@misc{nvml,
  title= {{NVIDIA Management Library (NVML)}},
  note = {\url{https://developer.nvidia.com/nvidia-management-library-nvml}, Accessed: May 2021}
}

@misc{bosp,
  title= {{The Barbeque Run-Time Resource Manager Open-Source Project (BOSP)}},
  note = {\url{https://bosp.deib.polimi.it/}, Accessed: May 2021}
}

@misc{xml,
  title= {{Extensible Markup Language (XML)}},
  note = {\url{https://www.w3.org/XML/}, Accessed: May 2021}
}

@misc{cuda_driver_api,
  title= {{CUDA Toolkit Documentation - CUDA Driver API}},
  note = {\url{https://docs.nvidia.com/cuda/cuda-driver-api/index.html}, Accessed: May 2021}
}

@misc{ptx,
  title= {{CUDA Toolkit Documentation - Parallel Thread Execution ISA}},
  note = {\url{https://docs.nvidia.com/cuda/parallel-thread-execution/index.html}, Accessed: May 2021}
}

@misc{nrvtc,
  title= {{CUDA Toolkit Documentation - NVRTC}},
  note = {\url{https://docs.nvidia.com/cuda/nvrtc/index.html}, Accessed: May 2021}
}

@article{kokkos,
  title   = {Kokkos: Enabling manycore performance portability through polymorphic memory access patterns},
  journal = {Journal of Parallel and Distributed Computing },
  volume  = {74},
  number  = {12},
  pages   = {3202 - 3216},
  year    = {2014},
  note    = {Domain-Specific Languages and High-Level Frameworks for High-Performance Computing },
  issn    = {0743-7315},
  doi     = {https://doi.org/10.1016/j.jpdc.2014.07.003},
  url     = {http://www.sciencedirect.com/science/article/pii/S0743731514001257},
  author  = {H. Carter Edwards and Christian R. Trott and Daniel Sunderland}
}

@inproceedings{performance_portability_2019,
  author    = {T. {Deakin} and S. {McIntosh-Smith} and J. {Price} and A. {Poenaru} and P. {Atkinson} and C. {Popa} and J. {Salmon}},
  booktitle = {2019 IEEE/ACM International Workshop on Performance, Portability and Productivity in HPC (P3HPC)},
  title     = {Performance Portability across Diverse Computer Architectures},
  year      = {2019},
  volume    = {},
  number    = {},
  pages     = {1-13},
  doi       = {10.1109/P3HPC49587.2019.00006}
}

@article{performance_portability_2013,
  title    = {An investigation of the performance portability of OpenCL},
  journal  = {Journal of Parallel and Distributed Computing},
  volume   = {73},
  number   = {11},
  pages    = {1439-1450},
  year     = {2013},
  note     = {Novel architectures for high-performance computing},
  issn     = {0743-7315},
  doi      = {https://doi.org/10.1016/j.jpdc.2012.07.005},
  url      = {https://www.sciencedirect.com/science/article/pii/S0743731512001669},
  author   = {S.J. Pennycook and S.D. Hammond and S.A. Wright and J.A. Herdman and I. Miller and S.A. Jarvis},
  keywords = {Many-core computing, GPU computing, Optimisation, OpenCL, High performance computing},
  abstract = {This paper reports on the development of an MPI/OpenCL implementation of LU, an application-level benchmark from the NAS Parallel Benchmark Suite. An account of the design decisions addressed during the development of this code is presented, demonstrating the importance of memory arrangement and work-item/work-group distribution strategies when applications are deployed on different device types. The resulting platform-agnostic, single source application is benchmarked on a number of different architectures, and is shown to be 1.3–1.5× slower than native FORTRAN 77 or CUDA implementations on a single node and 1.3–3.1× slower on multiple nodes. We also explore the potential performance gains of OpenCL’s device fissioning capability, demonstrating up to a 3× speed-up over our original OpenCL implementation.}
}

@article{optimizing_opencl_fpga_integer,
  author  = {N. {Paulino} and J. C. {Ferreira} and J. M. P. {Cardoso}},
  journal = {IEEE Access},
  title   = {Optimizing OpenCL Code for Performance on FPGA: k-Means Case Study With Integer Data Sets},
  year    = {2020},
  volume  = {8},
  number  = {},
  pages   = {152286-152304},
  doi     = {10.1109/ACCESS.2020.3017552}
}

@inproceedings{performance_portability_2020,
  author    = {C. {Bertoni} and J. {Kwack} and T. {Applencourt} and Y. {Ghadar} and B. {Homerding} and C. {Knight} and B. {Videau} and H. {Zheng} and V. {Morozov} and S. {Parker}},
  booktitle = {2020 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)},
  title     = {Performance Portability Evaluation of OpenCL Benchmarks across Intel and NVIDIA Platforms},
  year      = {2020},
  volume    = {},
  number    = {},
  pages     = {330-339},
  doi       = {10.1109/IPDPSW50202.2020.00067}
}

@inproceedings{optimizing_opencl_fpga_automata,
  author    = {M. {Nourian} and M. E. {Zarch} and M. {Becchi}},
  booktitle = {2020 IEEE 26th International Conference on Parallel and Distributed Systems (ICPADS)},
  title     = {Optimizing Complex OpenCL Code for FPGA: A Case Study on Finite Automata Traversal},
  year      = {2020},
  volume    = {},
  number    = {},
  pages     = {518-527},
  doi       = {10.1109/ICPADS51040.2020.00073}
}

@article{solid_modeling_ikonas,
  author     = {Van Hook, Tim},
  title      = {Real-Time Shaded NC Milling Display},
  year       = {1986},
  issue_date = {Aug. 1986},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {20},
  number     = {4},
  issn       = {0097-8930},
  url        = {https://doi.org/10.1145/15886.15887},
  doi        = {10.1145/15886.15887},
  abstract   = {The real-time shaded display of a solid model being milled by a cutting tool following an NC path is attained by the image-space Boolean subtraction of solid objects. The technique is suitable for implementation in microcode in a raster graphic display processor. Update rates of 10 cutting operations per second are typical.},
  journal    = {SIGGRAPH Comput. Graph.},
  month      = aug,
  pages      = {15–20},
  numpages   = {6}
}

@inproceedings{early_matrix_multiplication_gpgpu,
  author    = {Larsen, E. Scott and McAllister, David},
  title     = {Fast Matrix Multiplies Using Graphics Hardware},
  year      = {2001},
  isbn      = {158113293X},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/582034.582089},
  doi       = {10.1145/582034.582089},
  abstract  = {We present a technique for large matrix-matrix multiplies using low cost graphics hardware. The result is computed by literally visualizing the computations of a simple parallel processing algorithm. Current graphics hardware technology has limited precision and thus limits immediate applicability of our algorithm. We include results demonstrating proof of concept, correctness, speedup, and a simple application. This is therefore forward looking research: a technique ready for technology on the horizon.},
  booktitle = {Proceedings of the 2001 ACM/IEEE Conference on Supercomputing},
  pages     = {55},
  numpages  = {1},
  keywords  = {graphics hardware, matrix multiplication},
  location  = {Denver, Colorado},
  series    = {SC '01}
}

@inproceedings{unix_passwords_gpgpu,
  author    = {Gershon Kedem and Yuriko Ishihara},
  title     = {Brute Force Attack on {UNIX} Passwords with {SIMD} Computer},
  booktitle = {8th {USENIX} Security Symposium ({USENIX} Security 99)},
  year      = {1999},
  address   = {Washington, D.C.},
  url       = {https://www.usenix.org/conference/8th-usenix-security-symposium/brute-force-attack-unix-passwords-simd-computer},
  publisher = {{USENIX} Association},
  month     = aug
}

@inproceedings{voronoi_diagrams_gpgpu,
  author    = {Hoff, Kenneth E. and Keyser, John and Lin, Ming and Manocha, Dinesh and Culver, Tim},
  title     = {Fast Computation of Generalized Voronoi Diagrams Using Graphics Hardware},
  year      = {1999},
  isbn      = {0201485605},
  publisher = {ACM Press/Addison-Wesley Publishing Co.},
  address   = {USA},
  url       = {https://doi.org/10.1145/311535.311567},
  doi       = {10.1145/311535.311567},
  booktitle = {Proceedings of the 26th Annual Conference on Computer Graphics and Interactive Techniques},
  pages     = {277–286},
  numpages  = {10},
  keywords  = {voronoi diagrams, proximity query, interpolation, medial axis, OpenGL, motion planning, framebuffer techniques, polygon rasterization, graphics hardware},
  series    = {SIGGRAPH '99}
}

@inproceedings{physics_simulations_gpgpu,
  author    = {Harris, Mark J. and Coombe, Greg and Scheuermann, Thorsten and Lastra, Anselmo},
  title     = {Physically-Based Visual Simulation on Graphics Hardware},
  year      = {2002},
  isbn      = {1581135807},
  publisher = {Eurographics Association},
  address   = {Goslar, DEU},
  abstract  = {In this paper, we present a method for real-time visual simulation of diverse dynamic phenomena using programmable graphics hardware. The simulations we implement use an extension of cellular automata known as the coupled map lattice (CML). CML represents the state of a dynamic system as continuous values on a discrete lattice. In our implementation we store the lattice values in a texture, and use pixel-level programming to implement simple next-state computations on lattice nodes and their neighbors. We apply these computations successively to produce interactive visual simulations of convection, reaction-diffusion, and boiling. We have built an interactive framework for building and experimenting with CML simulations running on graphics hardware, and have integrated them into interactive 3D graphics applications.},
  booktitle = {Proceedings of the ACM SIGGRAPH/EUROGRAPHICS Conference on Graphics Hardware},
  pages     = {109–118},
  numpages  = {10},
  keywords  = {coupled map lattice, CML, visual simulation, reaction-diffusion, multipass rendering, graphics hardware},
  location  = {Saarbrucken, Germany},
  series    = {HWWS '02}
}

@article{ray_tracing_gpgpu,
  author     = {Purcell, Timothy J. and Buck, Ian and Mark, William R. and Hanrahan, Pat},
  title      = {Ray Tracing on Programmable Graphics Hardware},
  year       = {2002},
  issue_date = {July 2002},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {21},
  number     = {3},
  issn       = {0730-0301},
  url        = {https://doi.org/10.1145/566654.566640},
  doi        = {10.1145/566654.566640},
  abstract   = {Recently a breakthrough has occurred in graphics hardware: fixed function pipelines have been replaced with programmable vertex and fragment processors. In the near future, the graphics pipeline is likely to evolve into a general programmable stream processor capable of more than simply feed-forward triangle rendering.In this paper, we evaluate these trends in programmability of the graphics pipeline and explain how ray tracing can be mapped to graphics hardware. Using our simulator, we analyze the performance of a ray casting implementation on next generation programmable graphics hardware. In addition, we compare the performance difference between non-branching programmable hardware using a multipass implementation and an architecture that supports branching. We also show how this approach is applicable to other ray tracing algorithms such as Whitted ray tracing, path tracing, and hybrid rendering algorithms. Finally, we demonstrate that ray tracing on graphics hardware could prove to be faster than CPU based implementations as well as competitive with traditional hardware accelerated feed-forward triangle rendering.},
  journal    = {ACM Trans. Graph.},
  month      = jul,
  pages      = {703–712},
  numpages   = {10},
  keywords   = {ray tracing, programmable graphics hardware}
}

@inproceedings{photon_mapping_gpgpu,
  author    = {Purcell, Timothy J. and Donner, Craig and Cammarano, Mike and Jensen, Henrik Wann and Hanrahan, Pat},
  title     = {Photon Mapping on Programmable Graphics Hardware},
  year      = {2003},
  isbn      = {1581137397},
  publisher = {Eurographics Association},
  address   = {Goslar, DEU},
  abstract  = {We present a modified photon mapping algorithm capable of running entirely on GPUs. Our implementation uses breadth-first photon tracing to distribute photons using the GPU. The photons are stored in a grid-based photon map that is constructed directly on the graphics hardware using one of two methods: the first method is a multipass technique that uses fragment programs to directly sort the photons into a compact grid. The second method uses a single rendering pass combining a vertex program and the stencil buffer to route photons to their respective grid cells, producing an approximate photon map. We also present an efficient method for locating the nearest photons in the grid, which makes it possible to compute an estimate of the radiance at any surface location in the scene. Finally, we describe a breadth-first stochastic ray tracer that uses the photon map to simulate full global illumination directly on the graphics hardware. Our implementation demonstrates that current graphics hardware is capable of fully simulating global illumination with progressive, interactive feedback to the user.},
  booktitle = {Proceedings of the ACM SIGGRAPH/EUROGRAPHICS Conference on Graphics Hardware},
  pages     = {41–50},
  numpages  = {10},
  keywords  = {programmable graphics hardware, global illumination, photon mapping},
  location  = {San Diego, California},
  series    = {HWWS '03}
}

@inproceedings{nvidia_cg,
	author    = {Mark, William R. and Glanville, R. Steven and Akeley, Kurt and Kilgard, Mark J.},
	title     = {Cg: A System for Programming Graphics Hardware in a C-like Language},
	year      = {2003},
	isbn      = {1581137095},
	publisher = {Association for Computing Machinery},
	address   = {New York, NY, USA},
	url       = {https://doi.org/10.1145/1201775.882362},
	doi       = {10.1145/1201775.882362},
	abstract  = {The latest real-time graphics architectures include programmable floating-point vertex and fragment processors, with support for data-dependent control flow in the vertex processor. We present a programming language and a supporting system that are designed for programming these stream processors. The language follows the philosophy of C, in that it is a hardware-oriented, general-purpose language, rather than an application-specific shading language. The language includes a variety of facilities designed to support the key architectural features of programmable graphics processors, and is designed to support multiple generations of graphics architectures with different levels of functionality. The system supports both of the major 3D graphics APIs: OpenGL and Direct3D. This paper identifies many of the choices that we faced as we designed the system, and explains why we made the decisions that we did.},
	booktitle = {ACM SIGGRAPH 2003 Papers},
	pages     = {896–907},
	numpages  = {12},
	location  = {San Diego, California},
	series    = {SIGGRAPH '03}
}

@article{gpu_computing,
  author  = {Owens, John D. and Houston, Mike and Luebke, David and Green, Simon and Stone, John E. and Phillips, James C.},
  journal = {Proceedings of the IEEE},
  title   = {GPU Computing},
  year    = {2008},
  volume  = {96},
  number  = {5},
  pages   = {879-899},
  doi     = {10.1109/JPROC.2008.917757}
}

@article{brook,
  author     = {Buck, Ian and Foley, Tim and Horn, Daniel and Sugerman, Jeremy and Fatahalian, Kayvon and Houston, Mike and Hanrahan, Pat},
  title      = {Brook for GPUs: Stream Computing on Graphics Hardware},
  year       = {2004},
  issue_date = {August 2004},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {23},
  number     = {3},
  issn       = {0730-0301},
  url        = {https://doi.org/10.1145/1015706.1015800},
  doi        = {10.1145/1015706.1015800},
  abstract   = {In this paper, we present Brook for GPUs, a system for general-purpose computation on programmable graphics hardware. Brook extends C to include simple data-parallel constructs, enabling the use of the GPU as a streaming co-processor. We present a compiler and runtime system that abstracts and virtualizes many aspects of graphics hardware. In addition, we present an analysis of the effectiveness of the GPU as a compute engine compared to the CPU, to determine when the GPU can outperform the CPU for a particular algorithm. We evaluate our system with five applications, the SAXPY and SGEMV BLAS operators, image segmentation, FFT, and ray tracing. For these applications, we demonstrate that our Brook implementations perform comparably to hand-written GPU code and up to seven times faster than their CPU counterparts.},
  journal    = {ACM Trans. Graph.},
  month      = aug,
  pages      = {777–786},
  numpages   = {10},
  keywords   = {GPU Computing, Data Parallel Computing, Stream Computing, Brook, Programmable Graphics Hardware}
}

@inproceedings{investigation_sycl,
  title  = {Investigation of the OpenCL SYCL Programming Model},
  author = {Angelos Trigkas},
  month  = {Aug},
  year   = {2014}
}

@inproceedings{performance_portability_multimaterial_kernels,
  author    = {Reguly, István Z.},
  booktitle = {2019 IEEE/ACM International Workshop on Performance, Portability and Productivity in HPC (P3HPC)},
  title     = {Performance Portability of Multi-Material Kernels},
  year      = {2019},
  volume    = {},
  number    = {},
  pages     = {26-35},
  doi       = {10.1109/P3HPC49587.2019.00008}
}


@inproceedings{sycl_hpc_applications,
  title     = {Evaluating the performance of HPC-style SYCL applications},
  abstract  = {SYCL is a parallel programming model for developing single-source programs for running on heterogeneous platforms. To this end, it allows for one code to be written which can run on a different architectures. For this study, we develop applications in SYCL which are representative of those often used in High-Performance Computing. Their performance is benchmarked on a variety of CPU and GPU architectures from multiple vendors, and compared to well optimised versions written in OpenCL and other parallel programming models.},
  keywords  = {SYCL, GPGPUs, performance portability, benchmarking},
  author    = {Tom Deakin and Simon McIntosh-Smith},
  year      = {2020},
  month     = apr,
  day       = {1},
  doi       = {10.1145/3388333.3388643},
  language  = {English},
  booktitle = {IWOCL '20},
  publisher = {Association for Computing Machinery (ACM)},
  address   = {United States},
  note      = {International Workshop on OpenCL, IWOCL ; Conference date: 27-04-2020 Through 29-04-2020}
}

@inproceedings{celerity,
  author    = {Thoman, Peter 
  and Salzmann, Philip 
  and Cosenza, Biagio 
  and Fahringer, Thomas},
  editor    = {Yahyapour, Ramin},
  title     = {Celerity: High-Level C++ for Accelerator Clusters},
  booktitle = {Euro-Par 2019: Parallel Processing},
  year      = {2019},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {291--303},
  abstract  = {In the face of ever-slowing single-thread performance growth for CPUs, the scientific and engineering communities increasingly turn to accelerator parallelization to tackle growing application workloads. Existing means of targeting distributed memory accelerator clusters impose severe programmability barriers and maintenance burdens.},
  isbn      = {978-3-030-29400-7}
}

@inproceedings{mpi,
  author    = {Clarke, Lyndon
  and Glendinning, Ian
  and Hempel, Rolf},
  editor    = {Decker, Karsten M.
  and Rehmann, Ren{\'e} M.},
  title     = {The MPI Message Passing Interface Standard},
  booktitle = {Programming Environments for Massively Parallel Distributed Systems},
  year      = {1994},
  publisher = {Birkh{\"a}user Basel},
  address   = {Basel},
  pages     = {213--218},
  abstract  = {The diverse message passing interfaces provided on parallel and distributed computing systems have caused difficulty in movement of application software from one system to another and have inhibited the commercial development of tools and libraries for these systems. The Message Passing Interface (MPI) Forum has developed a de facto interface standard which was finalised in Ql of 1994. Major parallel system vendors and software developers were involved in the definition process, and the first implementations of MPI are already appearing. This article presents an overview of the MPI initiative and the standard interface, in particular those aspects which merge demonstrated research with common practice.},
  isbn      = {978-3-0348-8534-8}
}

@article{hpx,
  author  = {Kaiser, Hartmut and Diehl, Patrick and Lemoine, Adrian and Lelbach, Bryce and Amini, Parsa and Berge, Agustín and Biddiscombe, John and Brandt, Steven and Gupta, Nikunj and Heller, Thomas and Huck, Kevin and Khatami, Zahra and Kheirkhanan, Alireza and Reverdell, Auriane and Shirzad, Shahrzad and Simberg, Mikael and Wagle, Bibek and Wei, Weile and Zhang, Tianyi},
  year    = {2020},
  month   = {09},
  pages   = {2352},
  title   = {HPX - The C++ Standard Library for Parallelism and Concurrency},
  volume  = {5},
  journal = {Journal of Open Source Software},
  doi     = {10.21105/joss.02352}
}

@unknown{ginkgo_rocm_port,
  author = {Tsai, Yuhsiang and Cojean, Terry and Ribizel, Tobias and Anzt, Hartwig},
  year   = {2020},
  month  = {06},
  pages  = {},
  title  = {Preparing Ginkgo for AMD GPUs -- A Testimonial on Porting CUDA Code to HIP}
}

@article{molecular_dynamics_rocm_port,
  author   = {Nikolay Kondratyuk and Vsevolod Nikolskiy and Daniil Pavlov and Vladimir Stegailov},
  title    = {GPU-accelerated molecular dynamics: State-of-art software performance and porting from Nvidia CUDA to AMD HIP},
  journal  = {The International Journal of High Performance Computing Applications},
  volume   = {0},
  number   = {0},
  pages    = {10943420211008288},
  year     = {0},
  doi      = {10.1177/10943420211008288},
  url      = {https://doi.org/10.1177/10943420211008288},
  eprint   = {https://doi.org/10.1177/10943420211008288},
  abstract = { Classical molecular dynamics (MD) calculations represent a significant part of the utilization time of high-performance computing systems. As usual, the efficiency of such calculations is based on an interplay of software and hardware that are nowadays moving to hybrid GPU-based technologies. Several well-developed open-source MD codes focused on GPUs differ both in their data management capabilities and in performance. In this work, we analyze the performance of LAMMPS, GROMACS and OpenMM MD packages with different GPU backends on Nvidia Volta and AMD Vega20 GPUs. We consider the efficiency of solving two identical MD models (generic for material science and biomolecular studies) using different software and hardware combinations. We describe our experience in porting the CUDA backend of LAMMPS to ROCm HIP that shows considerable benefits for AMD GPUs comparatively to the OpenCL backend. }
}


@inproceedings{amd_as_alternative,
  author    = {Nathan Otterness and James H. Anderson},
  title     = {{AMD GPUs as an Alternative to NVIDIA for Supporting Real-Time Workloads}},
  booktitle = {32nd Euromicro Conference on Real-Time Systems (ECRTS 2020)},
  pages     = {10:1--10:23},
  series    = {Leibniz International Proceedings in Informatics (LIPIcs)},
  isbn      = {978-3-95977-152-8},
  issn      = {1868-8969},
  year      = {2020},
  volume    = {165},
  editor    = {Marcus V{\"o}lp},
  publisher = {Schloss Dagstuhl--Leibniz-Zentrum f{\"u}r Informatik},
  address   = {Dagstuhl, Germany},
  url       = {https://drops.dagstuhl.de/opus/volltexte/2020/12373},
  urn       = {urn:nbn:de:0030-drops-123732},
  doi       = {10.4230/LIPIcs.ECRTS.2020.10},
  annote    = {Keywords: real-time systems, graphics processing units, parallel computing}
}