@misc{opencl_overview,
	title = {{OpenCL: Overview}},
	note = {\url{https://www.khronos.org/opencl/}, Accessed: Apr 2021}
}

@misc{opencl_conformant_companies,
	title = {{OpenCL: API Adopters}},
	note = {\url{https://www.khronos.org/conformance/adopters/conformant-companies}, Accessed: Apr 2021}
}

@misc{opencl_spec,
	title = {{The OpenCL Specification v3.0.6}},
	year = {2020},
	month = {Dec},
	note = {\url{https://www.khronos.org/registry/OpenCL/specs/3.0-unified/pdf/OpenCL_API.pdf}}
}


@misc{opencl_c_spec,
	title = {{The OpenCL C Specification v3.0.6}},
	year = {2020},
	month = {Dec},
	note = {\url{https://www.khronos.org/registry/OpenCL/specs/3.0-unified/pdf/OpenCL_C.pdf}}
}

@misc{c99,
	title = {{ISO/IEC 9899:1999 - Programming languages - C}},
	year = {1999},
	month = {Dec}
}

@misc{c11,
	title = {{ISO/IEC 9899:2011 - Information technology - Programming languages - C}},
	year = {2011},
	month = {Dec}
}

@misc{openmp_spec,
	title = {{OpenMP API Specification 5.1}},
	year = {2020},
	month = {Nov},
	note = {\url{https://www.openmp.org/wp-content/uploads/OpenMP-API-Specification-5-1.pdf}}
}

@misc{openmp_gpu_support,
	title = {{OpenMP Accelerator Support for GPUs}},
	note = {\url{https://www.openmp.org/updates/openmp-accelerator-support-gpus/}, Accessed: Apr 2021}
}

@misc{openacc_spec,
	title = {{OpenACC API Specification 3.1}},
	year = {2020},
	month = {Nov},
	note = {\url{https://www.openacc.org/sites/default/files/inline-images/Specification/OpenACC-3.1-final.pdf}}
}

@misc{openacc_initial_spec,
	title = {{OpenACC API Specification 1.0}},
	year = {2011},
	month = {Nov},
	note = {\url{https://www.openacc.org/sites/default/files/inline-files/OpenACC_1_0_specification.pdf}}
}

@misc{spir_spec,
	title = {{SPIR-V Specification 1.5 Revision 5}},
	year = {2020},
	month = {Jan},
	note = {\url{https://www.khronos.org/registry/spir-v/specs/unified1/SPIRV.html}}
}

@misc{spir_overview,
	title = {{SPIR Overview}},
	note = {\url{https://www.khronos.org/spir/}, Accessed: Apr 2021}
}

@misc{vulkan,
	title = {{Vulkan}},
	note = {\url{https://www.khronos.org/vulkan/}, Accessed: Apr 2021}
}

@misc{brief_history_gpgpu,
	title = {{A Brief History of GPGPU}},
	author = {{Mark Harris}},
	month = {May},
	year = {2015}
}

@misc{ikonas,
	title = {{Ikonas Graphics Systems - The world's first GPGPU}},
	author = {{Nick England}},
	note = {\url{http://www.graphics-history.org/ikonas/}, Accessed: Apr 2021}
}

@misc{geforce_256,
	title = {{TechPowerUp: NVIDIA GeForce 256 SDR}},
	note = {\url{https://www.techpowerup.com/gpu-specs/geforce-256-sdr.c731}, Accessed: Apr 2021}
}

@misc{opengl,
	title = {{OpenGL}},
	note = {\url{https://www.opengl.org/}, Accessed: Apr 2021}
}

@misc{opengl_fixed_function_pipeline,
	title = {{OpenGL Wiki: Fixed Function Pipeline}},
	note = {\url{https://www.khronos.org/opengl/wiki/Fixed_Function_Pipeline}, Accessed: Apr 2021}
}

@inproceedings{transparent_cpu_gpu_collaboration,
	author = {Lee, Janghaeng and Samadi, Mehrzad and Park, Yongjun and Mahlke, Scott},
	title = {Transparent CPU-GPU Collaboration for Data-Parallel Kernels on Heterogeneous Systems},
	year = {2013},
	isbn = {9781479910212},
	publisher = {IEEE Press},
	abstract = {Heterogeneous computing on CPUs and GPUs has traditionally used fixed roles for each device: the GPU handles data parallel work by taking advantage of its massive number of cores while the CPU handles non data-parallel work, such as the sequential code or data transfer management. Unfortunately, this work distribution can be a poor solution as it under utilizes the CPU, has difficulty generalizing beyond the single CPU-GPU combination, and may waste a large fraction of time transferring data. Further, CPUs are performance competitive with GPUs on many workloads, thus simply partitioning work based on the fixed roles may be a poor choice. In this paper, we present the single kernel multiple devices (SKMD) system, a framework that transparently orchestrates collaborative execution of a single data-parallel kernel across multiple asymmetric CPUs and GPUs. The programmer is responsible for developing a single data-parallel kernel in OpenCL, while the system automatically partitions the workload across an arbitray set of devices, generates kernels to execute the partial workloads, and efficiently merges the partial outputs together. The goal is performance improvement by maximally utilizing all available resources to execute the kernel. SKMD handles the difficult challenges of exposed data transfer costs and the performance variations GPUs have with respect to input size. On real hardware, SKMD achieves an average speedup of 29% on a system with one multicore CPU and two asymmetric GPUs compared to a fastest device execution strategy for a set of popular OpenCL kernels.},
	booktitle = {Proceedings of the 22nd International Conference on Parallel Architectures and Compilation Techniques},
	pages = {245–256},
	numpages = {12},
	keywords = {collaboration, openCL, GPGPU, data parallel},
	location = {Edinburgh, Scotland, UK},
	series = {PACT '13}
}

@inproceedings{smart_multitasking_scheduling,
	author={Y. {Wen} and Z. {Wang} and M. F. P. {O'Boyle}},
	booktitle={2014 21st International Conference on High Performance Computing (HiPC)}, 
	title={Smart multi-task scheduling for OpenCL programs on CPU/GPU heterogeneous platforms}, 
	year={2014},
	volume={},
	number={},
	pages={1-10},
	doi={10.1109/HiPC.2014.7116910}
}

@Article{load_balance_model_opencl_integrated_cluster,
	author={Ahmed, Usman
	and Lin, Jerry Chun-Wei
	and Srivastava, Gautam
	and Aleem, Muhammad},
	title={A load balance multi-scheduling model for OpenCL kernel tasks in an integrated cluster},
	journal={Soft Computing},
	year={2021},
	month={Jan},
	day={01},
	volume={25},
	number={1},
	pages={407-420},
	abstract={Nowadays, embedded systems are comprised of heterogeneous multi-core architectures, i.e., CPUs and GPUs. If the application is mapped to an appropriate processing core, then these architectures provide many performance benefits to applications. Typically, programmers map sequential applications to CPU and parallel applications to GPU. The task mapping becomes challenging because of the usage of evolving and complex CPU- and GPU-based architectures. This paper presents an approach to map the OpenCL application to heterogeneous multi-core architecture by determining the application suitability and processing capability. The classification is achieved by developing a machine learning-based device suitability classifier that predicts which processor has the highest computational compatibility to run OpenCL applications. In this paper, 20 distinct features are proposed that are extracted by using the developed LLVM-based static analyzer. In order to select the best subset of features, feature selection is performed by using both correlation analysis and the feature importance method. For the class imbalance problem, we use and compare synthetic minority over-sampling method with and without feature selection. Instead of hand-tuning the machine learning classifier, we use the tree-based pipeline optimization method to select the best classifier and its hyper-parameter. We then compare the optimized selected method with traditional algorithms, i.e., random forest, decision tree, Na{\"i}ve Bayes and KNN. We apply our novel approach on extensively used OpenCL benchmarks, i.e., AMD and Polybench. The dataset contains 653 training and 277 testing applications. We test the classification results using four performance metrics, i.e., F-measure, precision, recall and {\$}{\$}R^2{\$}{\$}. The optimized and reduced feature subset model achieved a high F-measure of 0.91 and {\$}{\$}R^2{\$}{\$}of 0.76. The proposed framework automatically distributes the workload based on the application requirement and processor compatibility.},
	issn={1433-7479},
	doi={10.1007/s00500-020-05152-8},
	url={https://doi.org/10.1007/s00500-020-05152-8}
}

@article{dynamic_self_scheduling,
	author = {Belviranli, Mehmet E. and Bhuyan, Laxmi N. and Gupta, Rajiv},
	title = {A Dynamic Self-Scheduling Scheme for Heterogeneous Multiprocessor Architectures},
	year = {2013},
	issue_date = {January 2013},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {9},
	number = {4},
	issn = {1544-3566},
	url = {https://doi.org/10.1145/2400682.2400716},
	doi = {10.1145/2400682.2400716},
	abstract = {Today's heterogeneous architectures bring together multiple general-purpose CPUs and multiple domain-specific GPUs and FPGAs to provide dramatic speedup for many applications. However, the challenge lies in utilizing these heterogeneous processors to optimize overall application performance by minimizing workload completion time. Operating system and application development for these systems is in their infancy.In this article, we propose a new scheduling and workload balancing scheme, HDSS, for execution of loops having dependent or independent iterations on heterogeneous multiprocessor systems. The new algorithm dynamically learns the computational power of each processor during an adaptive phase and then schedules the remainder of the workload using a weighted self-scheduling scheme during the completion phase. Different from previous studies, our scheme uniquely considers the runtime effects of block sizes on the performance for heterogeneous multiprocessors. It finds the right trade-off between large and small block sizes to maintain balanced workload while keeping the accelerator utilization at maximum. Our algorithm does not require offline training or architecture-specific parameters.We have evaluated our scheme on two different heterogeneous architectures: AMD 64-core Bulldozer system with nVidia Fermi C2050 GPU and Intel Xeon 32-core SGI Altix 4700 supercomputer with Xilinx Virtex 4 FPGAs. The experimental results show that our new scheduling algorithm can achieve performance improvements up to over 200% when compared to the closest existing load balancing scheme. Our algorithm also achieves full processor utilization with all processors completing at nearly the same time which is significantly better than alternative current approaches.},
	journal = {ACM Trans. Archit. Code Optim.},
	month = jan,
	articleno = {57},
	numpages = {20},
	keywords = {workload balancing, FPGAs, GP-GPUs, Dynamic self-scheduling}
}

@article{enginecl,
	author = {Nozal, Raúl and Bosque, Jose and Beivide, Ramon},
	year = {2020},
	month = {02},
	pages = {},
	title = {EngineCL: Usability and Performance in Heterogeneous Computing},
	volume = {107},
	journal = {Future Generation Computer Systems},
	doi = {10.1016/j.future.2020.02.016}
}

@inproceedings{fluidicl,
	author = {Pandit, Prasanna and Govindarajan, R.},
	title = {Fluidic Kernels: Cooperative Execution of OpenCL Programs on Multiple Heterogeneous Devices},
	year = {2014},
	isbn = {9781450326704},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2581122.2544163},
	doi = {10.1145/2581122.2544163},
	abstract = {Programming heterogeneous computing systems with Graphics Processing Units (GPU) and multi-core CPUs in them is complex and time-consuming. OpenCL has emerged as an attractive programming framework for heterogeneous systems. But utilizing multiple devices in OpenCL is a challenge because it requires the programmer to explicitly map data and computation to each device. The problem becomes even more complex if the same OpenCL kernel has to be executed synergistically using multiple devices, as the relative execution time of the kernel on different devices can vary significantly, making it difficult to determine the work partitioning across these devices a priori. Also, after each kernel execution, a coherent version of the data needs to be established.In this work, we present FluidiCL, an OpenCL runtime that takes a program written for a single device and uses both the CPU and the GPU to execute it. Since we consider a setup with devices having discrete address spaces, our solution ensures that execution of OpenCL work-groups on devices is adjusted by taking into account the overheads for data management. The data transfers and data merging needed to ensure coherence are handled transparently without requiring any effort from the programmer. FluidiCL also does not require prior training or profiling and is completely portable across different machines. Across a set of diverse benchmarks having multiple kernels, our runtime shows a geomean speedup of nearly 64% over a high-end GPU and 88% over a 4-core CPU. In all benchmarks, performance of our runtime comes to within 13% of the best of the two devices.},
	booktitle = {Proceedings of Annual IEEE/ACM International Symposium on Code Generation and Optimization},
	pages = {273–283},
	numpages = {11},
	keywords = {Heterogeneous Devices, GPGPU, Runtime, OpenCL, FluidiCL},
	location = {Orlando, FL, USA},
	series = {CGO '14}
}

@inproceedings{survey_programming_models,
  author={B. {Pervan} and J. {Knezović}},
  booktitle={2020 43rd International Convention on Information, Communication and Electronic Technology (MIPRO)}, 
  title={A Survey on Parallel Architectures and Programming Models}, 
  year={2020},
  volume={},
  number={},
  pages={999-1005},
  doi={10.23919/MIPRO48935.2020.9245341}
}

@inproceedings{openmp_vs_openacc,
	author="Wienke, Sandra
	and Terboven, Christian
	and Beyer, James C.
	and M{\"u}ller, Matthias S.",
	editor="Silva, Fernando
	and Dutra, In{\^e}s
	and Santos Costa, V{\'i}tor",
	title="A Pattern-Based Comparison of OpenACC and OpenMP for Accelerator Computing",
	booktitle="Euro-Par 2014 Parallel Processing",
	year="2014",
	publisher="Springer International Publishing",
	address="Cham",
	pages="812--823",
	abstract="Nowadays, HPC systems frequently emerge as clusters of commodity processors with attached accelerators. Moving from tedious low-level accelerator programming to increased development productivity, the directive-based programming models OpenACC and OpenMP are promising candidates. While OpenACC was completed about two years ago, OpenMP just recently added support for accelerator programming. To assist developers in their decision-making which approach to take, we compare both models with respect to their programmability. Besides investigating their expressiveness by putting their constructs side by side, we focus on the evaluation of their power based on structured parallel programming patterns (aka algorithmic skeletons). These patterns describe the basic entities of parallel algorithms of which we cover the patterns map, stencil, reduction, fork-join, superscalar sequence, nesting and geometric decomposition. Architectural targets of this work are NVIDIA-type accelerators (GPUs) and specialties of Intel-type accelerators (Xeon Phis). Additionally, we assess the prospects of OpenACC and OpenMP concerning future development in soft- and hardware design.",
	isbn="978-3-319-09873-9"
}

@article{cuda_openacc_openmp_performance,
	doi = {10.1088/1742-6596/1740/1/012056},
	url = {https://doi.org/10.1088/1742-6596/1740/1/012056},
	year = 2021,
	month = {jan},
	publisher = {{IOP} Publishing},
	volume = {1740},
	pages = {012056},
	author = {Mikhail Khalilov and Alexey Timoveev},
	title = {Performance analysis of {CUDA}, {OpenACC} and {OpenMP} programming models on {TESLA} V100 {GPU}},
	journal = {Journal of Physics: Conference Series},
	abstract = {Graphics processors are widely utilized in modern supercomputers as accelerators. Ability to perform efficient parallelization and low-level allow scientists to greatly boost performance of their codes. Modern Nvidia GPUs feature low-level approaches, such as CUDA, along with high-level approaches: OpenACC and OpenMP. While the low-level approach aims to explore all possible abilities of SIMT GPU architecture by writing low-level C/C++ code, it takes significant effort from programmer. OpenACC and OpenMP programming models are opposite to CUDA. Using these models the programmer only have to identify the blocks of code to be parallelized using pragmas. We compare the performance of CUDA, OpenMP and OpenACC on state-of-the-art Nvidia Tesla V100 GPU in various typical scenarios that arise in scientific programming, such as matrix multiplication, regular memory access patterns and evaluate performance of physical simulation codes implemented using these programming models. Moreover, we study the performance matrix multiplication implemented in vendor-optimized BLAS libraries for Nvidia Tesla V100 GPU and modern Intel Xeon processor.}
}

@article{openmp_openacc_molecular_docking,
	author={Vitali, Emanuele
	and Gadioli, Davide
	and Palermo, Gianluca
	and Beccari, Andrea
	and Cavazzoni, Carlo
	and Silvano, Cristina},
	title={Exploiting OpenMP and OpenACC to accelerate a geometric approach to molecular docking in heterogeneous HPC nodes},
	journal={The Journal of Supercomputing},
	year={2019},
	month={Jul},
	day={01},
	volume={75},
	number={7},
	pages={3374-3396},
	abstract={In drug discovery, molecular docking is the task in charge of estimating the position of a molecule when interacting with the docking site. This task is usually used to perform screening of a large library of molecules, in the early phase of the process. Given the amount of candidate molecules and the complexity of the application, this task is usually performed using high-performance computing (HPC) platforms. In modern HPC systems, heterogeneous platforms provide a better throughput with respect to homogeneous platforms. In this work, we ported and optimized a molecular docking application to a heterogeneous system, with one or more GPU accelerators, leveraging a hybrid OpenMP and OpenACC approach. The target application focuses on the virtual screening phases in the drug discovery process, and it is based on geometric transformations of the target ligands. We prove that our approach has a better exploitation of the node compared to pure CPU/GPU data splitting approaches, reaching a throughput improvement up to 25{\%} while considering the same computing node.},
	issn={1573-0484},
	doi={10.1007/s11227-019-02875-w},
	url={https://doi.org/10.1007/s11227-019-02875-w}
}

@inproceedings{openmp_openacc_multigpus,
	author = {Xu, Rengan and Chandrasekaran, Sunita and Chapman, Barbara},
	year = {2013},
	month = {05},
	pages = {},
	title = {Exploring programming multi-GPUS using OpenMP and OpenACC-based hybrid model},
	journal = {Proceedings - IEEE 27th International Parallel and Distributed Processing Symposium Workshops and PhD Forum, IPDPSW 2013},
	doi = {10.1109/IPDPSW.2013.263}
}

@misc{kernel_wikipedia,
	title = {{Wikipedia: Compute Kernel}},
	note = {\url{https://en.wikipedia.org/wiki/Compute_kernel}, Accessed: Apr 2021}
}

@misc{buffer_wikipedia,
	title = {{Wikipedia: Compute Kernel}},
	note = {\url{https://en.wikipedia.org/wiki/Data_buffer}, Accessed: Apr 2021}
}

@inproceedings{mango_exploring_manycore_architectures,
	author = {G. Massari and G. Agosta and A. Pupykina and F. Reghenzani and M. Zanella and W. Fornaciari and J. Flich and R. Tornero and J. M. Martinez and M. Zapater and I. P. Fernandez and D. Atienza and A. Cilardo and M. Gagliardi and A. Dray and G. Guillaume},
	year = {2019},
	month = {02},
	pages = {},
	title = {MANGO - exploring Manycore Architectures for Next-GeneratiOn HPC Systems}
}


@article{kokkos,
  title = "Kokkos: Enabling manycore performance portability through polymorphic memory access patterns ",
  journal = "Journal of Parallel and Distributed Computing ",
  volume = "74",
  number = "12",
  pages = "3202 - 3216",
  year = "2014",
  note = "Domain-Specific Languages and High-Level Frameworks for High-Performance Computing ",
  issn = "0743-7315",
  doi = "https://doi.org/10.1016/j.jpdc.2014.07.003",
  url = "http://www.sciencedirect.com/science/article/pii/S0743731514001257",
  author = "H. Carter Edwards and Christian R. Trott and Daniel Sunderland"
}

@inproceedings{performance_portability_2019,
  author={T. {Deakin} and S. {McIntosh-Smith} and J. {Price} and A. {Poenaru} and P. {Atkinson} and C. {Popa} and J. {Salmon}},
  booktitle={2019 IEEE/ACM International Workshop on Performance, Portability and Productivity in HPC (P3HPC)}, 
  title={Performance Portability across Diverse Computer Architectures}, 
  year={2019},
  volume={},
  number={},
  pages={1-13},
  doi={10.1109/P3HPC49587.2019.00006}}

@article{performance_portability_2013,
  title = {An investigation of the performance portability of OpenCL},
  journal = {Journal of Parallel and Distributed Computing},
  volume = {73},
  number = {11},
  pages = {1439-1450},
  year = {2013},
  note = {Novel architectures for high-performance computing},
  issn = {0743-7315},
  doi = {https://doi.org/10.1016/j.jpdc.2012.07.005},
  url = {https://www.sciencedirect.com/science/article/pii/S0743731512001669},
  author = {S.J. Pennycook and S.D. Hammond and S.A. Wright and J.A. Herdman and I. Miller and S.A. Jarvis},
  keywords = {Many-core computing, GPU computing, Optimisation, OpenCL, High performance computing},
  abstract = {This paper reports on the development of an MPI/OpenCL implementation of LU, an application-level benchmark from the NAS Parallel Benchmark Suite. An account of the design decisions addressed during the development of this code is presented, demonstrating the importance of memory arrangement and work-item/work-group distribution strategies when applications are deployed on different device types. The resulting platform-agnostic, single source application is benchmarked on a number of different architectures, and is shown to be 1.3–1.5× slower than native FORTRAN 77 or CUDA implementations on a single node and 1.3–3.1× slower on multiple nodes. We also explore the potential performance gains of OpenCL’s device fissioning capability, demonstrating up to a 3× speed-up over our original OpenCL implementation.}
}

@article{optimizing_opencl_fpga_integer,
  author={N. {Paulino} and J. C. {Ferreira} and J. M. P. {Cardoso}},
  journal={IEEE Access}, 
  title={Optimizing OpenCL Code for Performance on FPGA: k-Means Case Study With Integer Data Sets}, 
  year={2020},
  volume={8},
  number={},
  pages={152286-152304},
  doi={10.1109/ACCESS.2020.3017552}
}

@inproceedings{performance_portability_2020,
  author={C. {Bertoni} and J. {Kwack} and T. {Applencourt} and Y. {Ghadar} and B. {Homerding} and C. {Knight} and B. {Videau} and H. {Zheng} and V. {Morozov} and S. {Parker}},
  booktitle={2020 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)}, 
  title={Performance Portability Evaluation of OpenCL Benchmarks across Intel and NVIDIA Platforms}, 
  year={2020},
  volume={},
  number={},
  pages={330-339},
  doi={10.1109/IPDPSW50202.2020.00067}
}

@inproceedings{optimizing_opencl_fpga_automata,
  author={M. {Nourian} and M. E. {Zarch} and M. {Becchi}},
  booktitle={2020 IEEE 26th International Conference on Parallel and Distributed Systems (ICPADS)}, 
  title={Optimizing Complex OpenCL Code for FPGA: A Case Study on Finite Automata Traversal}, 
  year={2020},
  volume={},
  number={},
  pages={518-527},
  doi={10.1109/ICPADS51040.2020.00073}
}

@article{solid_modeling_ikonas,
	author = {Van Hook, Tim},
	title = {Real-Time Shaded NC Milling Display},
	year = {1986},
	issue_date = {Aug. 1986},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {20},
	number = {4},
	issn = {0097-8930},
	url = {https://doi.org/10.1145/15886.15887},
	doi = {10.1145/15886.15887},
	abstract = {The real-time shaded display of a solid model being milled by a cutting tool following an NC path is attained by the image-space Boolean subtraction of solid objects. The technique is suitable for implementation in microcode in a raster graphic display processor. Update rates of 10 cutting operations per second are typical.},
	journal = {SIGGRAPH Comput. Graph.},
	month = aug,
	pages = {15–20},
	numpages = {6}
}

@inproceedings{early_matrix_multiplication_gpgpu,
	author = {Larsen, E. Scott and McAllister, David},
	title = {Fast Matrix Multiplies Using Graphics Hardware},
	year = {2001},
	isbn = {158113293X},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/582034.582089},
	doi = {10.1145/582034.582089},
	abstract = {We present a technique for large matrix-matrix multiplies using low cost graphics hardware. The result is computed by literally visualizing the computations of a simple parallel processing algorithm. Current graphics hardware technology has limited precision and thus limits immediate applicability of our algorithm. We include results demonstrating proof of concept, correctness, speedup, and a simple application. This is therefore forward looking research: a technique ready for technology on the horizon.},
	booktitle = {Proceedings of the 2001 ACM/IEEE Conference on Supercomputing},
	pages = {55},
	numpages = {1},
	keywords = {graphics hardware, matrix multiplication},
	location = {Denver, Colorado},
	series = {SC '01}
}

@inproceedings {unix_passwords_gpgpu,
	author = {Gershon Kedem and Yuriko Ishihara},
	title = {Brute Force Attack on {UNIX} Passwords with {SIMD} Computer},
	booktitle = {8th {USENIX} Security Symposium ({USENIX} Security 99)},
	year = {1999},
	address = {Washington, D.C.},
	url = {https://www.usenix.org/conference/8th-usenix-security-symposium/brute-force-attack-unix-passwords-simd-computer},
	publisher = {{USENIX} Association},
	month = aug,
}

@inproceedings{voronoi_diagrams_gpgpu,
	author = {Hoff, Kenneth E. and Keyser, John and Lin, Ming and Manocha, Dinesh and Culver, Tim},
	title = {Fast Computation of Generalized Voronoi Diagrams Using Graphics Hardware},
	year = {1999},
	isbn = {0201485605},
	publisher = {ACM Press/Addison-Wesley Publishing Co.},
	address = {USA},
	url = {https://doi.org/10.1145/311535.311567},
	doi = {10.1145/311535.311567},
	booktitle = {Proceedings of the 26th Annual Conference on Computer Graphics and Interactive Techniques},
	pages = {277–286},
	numpages = {10},
	keywords = {voronoi diagrams, proximity query, interpolation, medial axis, OpenGL, motion planning, framebuffer techniques, polygon rasterization, graphics hardware},
	series = {SIGGRAPH '99}
}